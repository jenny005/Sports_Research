{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzIyrPgOw+KewT8TPwMBh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny005/Sports_Research/blob/main/Baseball_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-GLJyjSVhBM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "End-to-end, self-contained baseball analytics pipeline.\n",
        "The module can be run as a script to demonstrate a full workflow:\n",
        "    1. Pull raw data from multiple modern sources (Statcast, wearable sensors, TrackMan).\n",
        "    2. Clean and enrich the data.\n",
        "    3. Train / update predictive models (player evaluation & in-game strategy).\n",
        "    4. Surface actionable recommendations via a lightweight REST service that any\n",
        "       department (scouting, player-dev, coaching staff) can consume.\n",
        "\n",
        "Usage (from project root):\n",
        "    $ python baseball_model.py --env dev --update-models --serve\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import datetime as dt\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "ENV = os.getenv(\"ENV\", \"dev\")\n",
        "ROOT = Path(__file__).resolve().parent\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "MODEL_DIR = ROOT / \"models\"\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "STATCAST_URL = \"https://baseballsavant.mlb.com/api/v1/people?season={year}\"\n",
        "WEARABLE_ENDPOINT = \"https://internals.myorg.com/api/wearable\"  # mock\n",
        "TRACKMAN_ENDPOINT = \"https://internals.myorg.com/api/trackman\"  # mock\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Data Acquisition\n",
        "# -----------------------------------------------------------------------------\n",
        "def pull_statcast(year: int) -> pd.DataFrame:\n",
        "    \"\"\"Pull public Statcast data for a given season.\"\"\"\n",
        "    url = STATCAST_URL.format(year=year)\n",
        "    resp = requests.get(url, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    raw = resp.json()\n",
        "    df = pd.json_normalize(raw[\"people\"])\n",
        "    return df.rename(columns=str.lower)\n",
        "\n",
        "\n",
        "def pull_wearable(player_ids: List[int]) -> pd.DataFrame:\n",
        "    \"\"\"Pull wearable sensor data (mock).\"\"\"\n",
        "    params = {\"player_ids\": \",\".join(map(str, player_ids))}\n",
        "    resp = requests.get(WEARABLE_ENDPOINT, params=params, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    return pd.DataFrame(resp.json()[\"data\"])\n",
        "\n",
        "\n",
        "def pull_trackman(player_ids: List[int]) -> pd.DataFrame:\n",
        "    \"\"\"Pull TrackMan pitch-level data (mock).\"\"\"\n",
        "    params = {\"player_ids\": \",\".join(map(str, player_ids))}\n",
        "    resp = requests.get(TRACKMAN_ENDPOINT, params=params, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    return pd.DataFrame(resp.json()[\"data\"])\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Data Cleaning & Enrichment\n",
        "# -----------------------------------------------------------------------------\n",
        "def clean_statcast(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Basic cleaning for Statcast.\"\"\"\n",
        "    # Drop rows with missing key metrics\n",
        "    df = df.dropna(subset=[\"war\", \"ops\", \"era\"])\n",
        "    # Ensure numeric\n",
        "    num_cols = [\"war\", \"ops\", \"era\", \"avg_exit_velocity\", \"max_exit_velocity\"]\n",
        "    for c in num_cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    # Fill remaining NaNs\n",
        "    df = df.fillna(df.median(numeric_only=True))\n",
        "    return df\n",
        "\n",
        "\n",
        "def merge_sources(\n",
        "    statcast: pd.DataFrame,\n",
        "    wearable: pd.DataFrame,\n",
        "    trackman: pd.DataFrame,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Merge multi-source data on player_id.\"\"\"\n",
        "    df = statcast.merge(\n",
        "        wearable, on=\"player_id\", how=\"left\", suffixes=(\"\", \"_wear\")\n",
        "    ).merge(trackman, on=\"player_id\", how=\"left\", suffixes=(\"\", \"_track\"))\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Feature Engineering\n",
        "# -----------------------------------------------------------------------------\n",
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create composite metrics.\"\"\"\n",
        "    df[\"hard_hit_pct\"] = df[\"hard_hit\"] / df[\"batted_events\"]\n",
        "    df[\"spin_efficiency\"] = df[\"spin_rate\"] / df[\"max_spin\"]\n",
        "    df[\"fatigue_index\"] = df[\"high_intensity_minutes\"] / df[\"total_minutes\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Modeling\n",
        "# -----------------------------------------------------------------------------\n",
        "TARGETS = {\n",
        "    \"war\": \"war\",\n",
        "    \"era\": \"era\",\n",
        "    \"ops\": \"ops\",\n",
        "}\n",
        "\n",
        "FEATURES = [\n",
        "    \"avg_exit_velocity\",\n",
        "    \"max_exit_velocity\",\n",
        "    \"launch_angle\",\n",
        "    \"spin_rate\",\n",
        "    \"hard_hit_pct\",\n",
        "    \"spin_efficiency\",\n",
        "    \"fatigue_index\",\n",
        "    \"position\",\n",
        "]\n",
        "\n",
        "CATEGORICAL = [\"position\"]\n",
        "NUMERICAL = [f for f in FEATURES if f not in CATEGORICAL]\n",
        "\n",
        "\n",
        "def build_pipeline() -> Pipeline:\n",
        "    \"\"\"Build sklearn pipeline for tabular data.\"\"\"\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL),\n",
        "            (\"num\", StandardScaler(), NUMERICAL),\n",
        "        ]\n",
        "    )\n",
        "    model = GradientBoostingRegressor(\n",
        "        n_estimators=400,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        random_state=42,\n",
        "    )\n",
        "    return Pipeline(steps=[(\"prep\", pre), (\"model\", model)])\n",
        "\n",
        "\n",
        "def train_models(\n",
        "    df: pd.DataFrame,\n",
        "    save: bool = True,\n",
        ") -> Dict[str, Pipeline]:\n",
        "    \"\"\"Train and evaluate models for each target.\"\"\"\n",
        "    results = {}\n",
        "    for name, y_col in TARGETS.items():\n",
        "        X = df[FEATURES]\n",
        "        y = df[y_col]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        pipe = build_pipeline()\n",
        "        pipe.fit(X_train, y_train)\n",
        "        preds = pipe.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, preds)\n",
        "        print(f\"{name} MAE: {mae:.3f}\")\n",
        "        results[name] = pipe\n",
        "        if save:\n",
        "            joblib.dump(pipe, MODEL_DIR / f\"{name}_model.joblib\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Recommendation Engine\n",
        "# -----------------------------------------------------------------------------\n",
        "class RecommendationRequest(BaseModel):\n",
        "    player_id: int\n",
        "    role: str  # evaluation | development | strategy\n",
        "\n",
        "\n",
        "class Recommendation(BaseModel):\n",
        "    player_id: int\n",
        "    role: str\n",
        "    metric: str\n",
        "    value: float\n",
        "    recommendation: str\n",
        "    confidence: float\n",
        "\n",
        "\n",
        "def generate_recommendation(\n",
        "    player_id: int,\n",
        "    role: str,\n",
        "    df: pd.DataFrame,\n",
        "    models: Dict[str, Pipeline],\n",
        ") -> Recommendation:\n",
        "    \"\"\"Create actionable recommendation for a single player.\"\"\"\n",
        "    row = df[df[\"player_id\"] == player_id]\n",
        "    if row.empty:\n",
        "        raise ValueError(\"Player not found\")\n",
        "    row = row.iloc[0]\n",
        "\n",
        "    # Choose metric based on role\n",
        "    if role == \"evaluation\":\n",
        "        target = \"war\"\n",
        "    elif role == \"development\":\n",
        "        target = \"ops\"\n",
        "    else:  # strategy\n",
        "        target = \"era\"\n",
        "\n",
        "    model = models[target]\n",
        "    X = row[FEATURES].to_frame().T\n",
        "    pred = model.predict(X)[0]\n",
        "    conf = 0.85  # placeholder; could use prediction intervals\n",
        "\n",
        "    # Simple rule-based recommendation\n",
        "    if target == \"war\" and pred < 2:\n",
        "        rec = \"Consider minor-league assignment for further development.\"\n",
        "    elif target == \"ops\" and pred < 0.700:\n",
        "        rec = \"Increase focus on exit velocity training.\"\n",
        "    elif target == \"era\" and pred > 4.5:\n",
        "        rec = \"Leverage high-spin fastball more often up in the zone.\"\n",
        "    else:\n",
        "        rec = \"Status quo; monitor progress.\"\n",
        "\n",
        "    return Recommendation(\n",
        "        player_id=player_id,\n",
        "        role=role,\n",
        "        metric=target,\n",
        "        value=pred,\n",
        "        recommendation=rec,\n",
        "        confidence=conf,\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. REST Service\n",
        "# -----------------------------------------------------------------------------\n",
        "app = FastAPI(title=\"Baseball Analytics API\", version=\"1.0\")\n",
        "models: Dict[str, Pipeline] = {}\n",
        "\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_models():\n",
        "    global models\n",
        "    for target in TARGETS:\n",
        "        path = MODEL_DIR / f\"{target}_model.joblib\"\n",
        "        if path.exists():\n",
        "            models[target] = joblib.load(path)\n",
        "\n",
        "\n",
        "@app.post(\"/recommend\", response_model=Recommendation)\n",
        "def get_recommendation(req: RecommendationRequest):\n",
        "    try:\n",
        "        df = joblib.load(DATA_DIR / \"current_data.joblib\")\n",
        "        rec = generate_recommendation(req.player_id, req.role, df, models)\n",
        "        return rec\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. Script entrypoint\n",
        "# -----------------------------------------------------------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--year\", type=int, default=dt.datetime.now().year - 1)\n",
        "    parser.add_argument(\"--update-models\", action=\"store_true\")\n",
        "    parser.add_argument(\"--serve\", action=\"store_true\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Pull data\n",
        "    print(\"Pulling data...\")\n",
        "    statcast = pull_statcast(args.year)\n",
        "    statcast = clean_statcast(statcast)\n",
        "    player_ids = statcast[\"player_id\"].unique().tolist()\n",
        "    wearable = pull_wearable(player_ids)\n",
        "    trackman = pull_trackman(player_ids)\n",
        "\n",
        "    # Merge & engineer\n",
        "    df = merge_sources(statcast, wearable, trackman)\n",
        "    df = engineer_features(df)\n",
        "    joblib.dump(df, DATA_DIR / \"current_data.joblib\")\n",
        "\n",
        "    # Train\n",
        "    if args.update_models:\n",
        "        print(\"Training models...\")\n",
        "        train_models(df)\n",
        "\n",
        "    # Serve\n",
        "    if args.serve:\n",
        "        import uvicorn\n",
        "\n",
        "        uvicorn.run(\"baseball_model:app\", host=\"0.0.0.0\", port=8000, reload=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}